{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eO_bVRH4vphHi6V4Oap3ioPYBzDTOQsS","timestamp":1678969871390}],"authorship_tag":"ABX9TyNnQro5gZSNm2brXNJ8LXp4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vPW-6akMZsFO"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"4강(02_end_to_end_machine_learning_project.ipynb) 과제\n","\n","Automatically generated by Colaboratory.\n","\n","**Chapter 2 – End-to-end Machine Learning project**\n","\n","*Welcome to Machine Learning Housing Corp.! Your task is to predict median house values in Californian districts, given a number of features from these districts.*\n","\n","*This notebook contains all the sample code and solutions to the exercices in chapter 2.*\n","\n","<table align=\"left\">\n","  <td>\n","    <a href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n","  </td>\n","</table>\n","\n","# Setup\n","\n","First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.\n","\"\"\""]},{"cell_type":"code","source":["# Commented out IPython magic to ensure Python compatibility.\n","# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)"],"metadata":{"id":"2Q-sgm7at1iV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\""],"metadata":{"id":"v2evKWMpt5gg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Common imports\n","import numpy as np\n","import os"],"metadata":{"id":"IpX-bH9Ot8YH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To plot pretty figures\n","# %matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)"],"metadata":{"id":"hXhgOT04t8nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"end_to_end_project\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)"],"metadata":{"id":"xfWJi5Jet8r3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"# Get the Data\n","\n","## Download the Data\n","\"\"\"\n","\n","import os\n","import tarfile\n","import urllib.request\n","\n","DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n","HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n","HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n","\n","def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n","    if not os.path.isdir(housing_path):\n","        os.makedirs(housing_path)\n","    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n","    urllib.request.urlretrieve(housing_url, tgz_path)\n","    housing_tgz = tarfile.open(tgz_path)\n","    housing_tgz.extractall(path=housing_path)\n","    housing_tgz.close()"],"metadata":{"id":"DOGC81MZt8uh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fetch_housing_data()"],"metadata":{"id":"Pq13c9THt8ys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","def load_housing_data(housing_path=HOUSING_PATH):\n","    csv_path = os.path.join(housing_path, \"housing.csv\")\n","    return pd.read_csv(csv_path)"],"metadata":{"id":"lO3BsAFIt80-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"## Take a Quick Look at the Data Structure\"\"\"\n","\n","housing = load_housing_data()\n","housing.head()\n","\n","housing.info()\n","\n","housing[\"ocean_proximity\"].value_counts()\n","\n","housing.describe()"],"metadata":{"id":"__5Yl09St83e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Commented out IPython magic to ensure Python compatibility.\n","# %matplotlib inline\n","import matplotlib.pyplot as plt\n","housing.hist(bins=50, figsize=(20,15))\n","save_fig(\"attribute_histogram_plots\")\n","plt.show()"],"metadata":{"id":"N10NNzK_t87i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"## Create a Test Set\"\"\"\n","\n","# to make this notebook's output identical at every run\n","np.random.seed(42)\n","\n","import numpy as np\n","\n","# For illustration only. Sklearn has train_test_split()\n","def split_train_test(data, test_ratio):\n","    shuffled_indices = np.random.permutation(len(data))\n","    test_set_size = int(len(data) * test_ratio)\n","    test_indices = shuffled_indices[:test_set_size]\n","    train_indices = shuffled_indices[test_set_size:]\n","    return data.iloc[train_indices], data.iloc[test_indices]\n","\n","train_set, test_set = split_train_test(housing, 0.2)\n","len(train_set)\n","\n","len(test_set)"],"metadata":{"id":"DblfBFRqt8_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from zlib import crc32\n","\n","def test_set_check(identifier, test_ratio):\n","    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n","\n","def split_train_test_by_id(data, test_ratio, id_column):\n","    ids = data[id_column]\n","    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n","    return data.loc[~in_test_set], data.loc[in_test_set]"],"metadata":{"id":"ToD8WmQet9Cf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"The implementation of `test_set_check()` above works fine in both Python 2 and Python 3. In earlier releases, the following implementation was proposed, which supported any hash function, but was much slower and did not support Python 2:\"\"\"\n","\n","import hashlib\n","\n","def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n","    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n","\n","\"\"\"If you want an implementation that supports any hash function and is compatible with both Python 2 and Python 3, here is one:\"\"\"\n","\n","def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n","    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n","\n","housing_with_id = housing.reset_index()   # adds an `index` column\n","train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n","\n","housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n","train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n","\n","test_set.head()\n"],"metadata":{"id":"_z7zoKaNukdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n","\n","test_set.head()\n","\n","housing[\"median_income\"].hist()\n","\n","housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n","                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n","                               labels=[1, 2, 3, 4, 5])\n","\n","housing[\"income_cat\"].value_counts()\n","\n","housing[\"income_cat\"].hist()\n"],"metadata":{"id":"TiwrXePHukiH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedShuffleSplit\n","\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n","for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n","    strat_train_set = housing.loc[train_index]\n","    strat_test_set = housing.loc[test_index]\n","\n","strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n","\n","housing[\"income_cat\"].value_counts() / len(housing)\n","\n","def income_cat_proportions(data):\n","    return data[\"income_cat\"].value_counts() / len(data)\n","\n","train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n","\n","compare_props = pd.DataFrame({\n","    \"Overall\": income_cat_proportions(housing),\n","    \"Stratified\": income_cat_proportions(strat_test_set),\n","    \"Random\": income_cat_proportions(test_set),\n","}).sort_index()\n","compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n","compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n","\n","compare_props\n","\n","for set_ in (strat_train_set, strat_test_set):\n","    set_.drop(\"income_cat\", axis=1, inplace=True)\n","\n","\"\"\"# Discover and Visualize the Data to Gain Insights\"\"\"\n","\n","housing = strat_train_set.copy()\n","\n","\"\"\"## Visualizing Geographical Data\"\"\"\n","\n","housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n","save_fig(\"bad_visualization_plot\")\n","\n","housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n","save_fig(\"better_visualization_plot\")\n","\n","\"\"\"The argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ). Thanks to Wilmer Arellano for pointing it out.\"\"\"\n","\n","housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n","             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n","             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n","             sharex=False)\n","plt.legend()\n","save_fig(\"housing_prices_scatterplot\")\n"],"metadata":{"id":"d9rrlKYOukmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download the California image\n","images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"end_to_end_project\")\n","os.makedirs(images_path, exist_ok=True)\n","DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n","filename = \"california.png\"\n","print(\"Downloading\", filename)\n","url = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\n","urllib.request.urlretrieve(url, os.path.join(images_path, filename))\n","\n","import matplotlib.image as mpimg\n","california_img=mpimg.imread(os.path.join(images_path, filename))\n","ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n","                  s=housing['population']/100, label=\"Population\",\n","                  c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n","                  colorbar=False, alpha=0.4)\n","plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n","           cmap=plt.get_cmap(\"jet\"))\n","plt.ylabel(\"Latitude\", fontsize=14)\n","plt.xlabel(\"Longitude\", fontsize=14)\n","\n","prices = housing[\"median_house_value\"]\n","tick_values = np.linspace(prices.min(), prices.max(), 11)\n","cbar = plt.colorbar(ticks=tick_values/prices.max())\n","cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n","cbar.set_label('Median House Value', fontsize=16)\n","\n","plt.legend(fontsize=16)\n","save_fig(\"california_housing_prices_plot\")\n","plt.show()\n","\n","\"\"\"## Looking for Correlations\"\"\"\n","\n","corr_matrix = housing.corr()\n","\n","corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n"],"metadata":{"id":"jvFN1OghukpH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n","from pandas.plotting import scatter_matrix\n","\n","attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n","              \"housing_median_age\"]\n","scatter_matrix(housing[attributes], figsize=(12, 8))\n","save_fig(\"scatter_matrix_plot\")\n","\n","housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n","             alpha=0.1)\n","plt.axis([0, 16, 0, 550000])\n","save_fig(\"income_vs_house_value_scatterplot\")\n","\n","\"\"\"## Experimenting with Attribute Combinations\"\"\"\n","\n","housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n","housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n","housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n","\n","corr_matrix = housing.corr()\n","corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n","\n","housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n","             alpha=0.2)\n","plt.axis([0, 5, 0, 520000])\n","plt.show()\n","\n","housing.describe()\n","\n","\"\"\"# Prepare the Data for Machine Learning Algorithms\"\"\"\n","\n","housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n","housing_labels = strat_train_set[\"median_house_value\"].copy()\n","\n","\"\"\"## Data Cleaning\n","\n","In the book 3 options are listed:\n","\n","```python\n","housing.dropna(subset=[\"total_bedrooms\"])    # option 1\n","housing.drop(\"total_bedrooms\", axis=1)       # option 2\n","median = housing[\"total_bedrooms\"].median()  # option 3\n","housing[\"total_bedrooms\"].fillna(median, inplace=True)\n","```\n","\n","To demonstrate each of them, let's create a copy of the housing dataset, but keeping only the rows that contain at least one null. Then it will be easier to visualize exactly what each option does:\n","\"\"\"\n","\n","sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n","sample_incomplete_rows\n","\n","sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1\n","\n","sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2\n","\n","median = housing[\"total_bedrooms\"].median()\n","sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\n","\n","sample_incomplete_rows\n"],"metadata":{"id":"PnaVBw6Fuktk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(strategy=\"median\")\n","\n","\"\"\"Remove the text attribute because median can only be calculated on numerical attributes:\"\"\"\n","\n","housing_num = housing.drop(\"ocean_proximity\", axis=1)\n","# alternatively: housing_num = housing.select_dtypes(include=[np.number])\n","\n","imputer.fit(housing_num)\n","\n","imputer.statistics_\n","\n","\"\"\"Check that this is the same as manually computing the median of each attribute:\"\"\"\n","\n","housing_num.median().values\n","\n","\"\"\"Transform the training set:\"\"\"\n","\n","X = imputer.transform(housing_num)\n","\n","housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n","                          index=housing.index)\n","\n","housing_tr.loc[sample_incomplete_rows.index.values]\n","\n","imputer.strategy\n","\n","housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n","                          index=housing_num.index)\n","\n","housing_tr.head()\n","\n","\"\"\"## Handling Text and Categorical Attributes\n","\n","Now let's preprocess the categorical input feature, `ocean_proximity`:\n","\"\"\"\n","\n","housing_cat = housing[[\"ocean_proximity\"]]\n","housing_cat.head(10)\n","\n","from sklearn.preprocessing import OrdinalEncoder\n","\n","ordinal_encoder = OrdinalEncoder()\n","housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n","housing_cat_encoded[:10]\n","\n","ordinal_encoder.categories_\n"],"metadata":{"id":"EVUTWbx7ukyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","\n","cat_encoder = OneHotEncoder()\n","housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n","housing_cat_1hot\n","\n","\"\"\"By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:\"\"\"\n","\n","housing_cat_1hot.toarray()\n","\n","\"\"\"Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:\"\"\"\n","\n","cat_encoder = OneHotEncoder(sparse=False)\n","housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n","housing_cat_1hot\n","\n","cat_encoder.categories_"],"metadata":{"id":"ZAfXkfHbuk0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"## Custom Transformers\n","\n","Let's create a custom transformer to add extra attributes:\n","\"\"\"\n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","# column index\n","rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n","\n","class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n","    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n","        self.add_bedrooms_per_room = add_bedrooms_per_room\n","    def fit(self, X, y=None):\n","        return self  # nothing else to do\n","    def transform(self, X):\n","        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n","        population_per_household = X[:, population_ix] / X[:, households_ix]\n","        if self.add_bedrooms_per_room:\n","            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n","            return np.c_[X, rooms_per_household, population_per_household,\n","                         bedrooms_per_room]\n","        else:\n","            return np.c_[X, rooms_per_household, population_per_household]\n","\n","attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n","housing_extra_attribs = attr_adder.transform(housing.values)\n","\n","\"\"\"Note that I hard coded the indices (3, 4, 5, 6) for concision and clarity in the book, but it would be much cleaner to get them dynamically, like this:\"\"\"\n","\n","col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\n","rooms_ix, bedrooms_ix, population_ix, households_ix = [\n","    housing.columns.get_loc(c) for c in col_names] # get the column indices\n","\n","\"\"\"Also, `housing_extra_attribs` is a NumPy array, we've lost the column names (unfortunately, that's a problem with Scikit-Learn). To recover a `DataFrame`, you could run this:\"\"\"\n","\n","housing_extra_attribs = pd.DataFrame(\n","    housing_extra_attribs,\n","    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n","    index=housing.index)\n","housing_extra_attribs.head()\n"],"metadata":{"id":"mxA7qijYuk5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"## Transformation Pipelines\n","\n","Now let's build a pipeline for preprocessing the numerical attributes:\n","\"\"\"\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","num_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy=\"median\")),\n","        ('attribs_adder', CombinedAttributesAdder()),\n","        ('std_scaler', StandardScaler()),\n","    ])\n","\n","housing_num_tr = num_pipeline.fit_transform(housing_num)\n","\n","housing_num_tr\n","\n","from sklearn.compose import ColumnTransformer\n","\n","num_attribs = list(housing_num)\n","cat_attribs = [\"ocean_proximity\"]\n","\n","full_pipeline = ColumnTransformer([\n","        (\"num\", num_pipeline, num_attribs),\n","        (\"cat\", OneHotEncoder(), cat_attribs),\n","    ])\n","\n","housing_prepared = full_pipeline.fit_transform(housing)\n","\n","housing_prepared\n","\n","housing_prepared.shape\n","\n","\"\"\"For reference, here is the old solution based on a `DataFrameSelector` transformer (to just select a subset of the Pandas `DataFrame` columns), and a `FeatureUnion`:\"\"\"\n"],"metadata":{"id":"4tyL8BFQuk9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","# Create a class to select numerical or categorical columns \n","class OldDataFrameSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, attribute_names):\n","        self.attribute_names = attribute_names\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return X[self.attribute_names].values\n","\n","\"\"\"Now let's join all these components into a big pipeline that will preprocess both the numerical and the categorical features:\"\"\"\n","\n","num_attribs = list(housing_num)\n","cat_attribs = [\"ocean_proximity\"]\n","\n","old_num_pipeline = Pipeline([\n","        ('selector', OldDataFrameSelector(num_attribs)),\n","        ('imputer', SimpleImputer(strategy=\"median\")),\n","        ('attribs_adder', CombinedAttributesAdder()),\n","        ('std_scaler', StandardScaler()),\n","    ])\n","\n","old_cat_pipeline = Pipeline([\n","        ('selector', OldDataFrameSelector(cat_attribs)),\n","        ('cat_encoder', OneHotEncoder(sparse=False)),\n","    ])\n"],"metadata":{"id":"fRHAjSpqvTOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.pipeline import FeatureUnion\n","\n","old_full_pipeline = FeatureUnion(transformer_list=[\n","        (\"num_pipeline\", old_num_pipeline),\n","        (\"cat_pipeline\", old_cat_pipeline),\n","    ])\n","\n","old_housing_prepared = old_full_pipeline.fit_transform(housing)\n","old_housing_prepared\n","\n","\"\"\"The result is the same as with the `ColumnTransformer`:\"\"\"\n","\n","np.allclose(housing_prepared, old_housing_prepared)"],"metadata":{"id":"dPCMNIG8vTTC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"# Select and Train a Model\n","\n","## Training and Evaluating on the Training Set\n","\"\"\"\n","\n","from sklearn.linear_model import LinearRegression\n","\n","lin_reg = LinearRegression()\n","lin_reg.fit(housing_prepared, housing_labels)\n","\n","# let's try the full preprocessing pipeline on a few training instances\n","some_data = housing.iloc[:5]\n","some_labels = housing_labels.iloc[:5]\n","some_data_prepared = full_pipeline.transform(some_data)\n","\n","print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n","\n","\"\"\"Compare against the actual values:\"\"\"\n","\n","print(\"Labels:\", list(some_labels))\n","\n","some_data_prepared\n","\n","from sklearn.metrics import mean_squared_error\n","\n","housing_predictions = lin_reg.predict(housing_prepared)\n","lin_mse = mean_squared_error(housing_labels, housing_predictions)\n","lin_rmse = np.sqrt(lin_mse)\n","lin_rmse\n","\n","\"\"\"**Note**: since Scikit-Learn 0.22, you can get the RMSE directly by calling the `mean_squared_error()` function with `squared=False`.\"\"\"\n"],"metadata":{"id":"rHSlM5hpvTWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_absolute_error\n","\n","lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n","lin_mae\n","\n","from sklearn.tree import DecisionTreeRegressor\n","\n","tree_reg = DecisionTreeRegressor(random_state=42)\n","tree_reg.fit(housing_prepared, housing_labels)\n","\n","housing_predictions = tree_reg.predict(housing_prepared)\n","tree_mse = mean_squared_error(housing_labels, housing_predictions)\n","tree_rmse = np.sqrt(tree_mse)\n","tree_rmse\n"],"metadata":{"id":"Kj-rVe1WvTZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"## Better Evaluation Using Cross-Validation\"\"\"\n","\n","from sklearn.model_selection import cross_val_score\n","\n","scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n","                         scoring=\"neg_mean_squared_error\", cv=10)\n","tree_rmse_scores = np.sqrt(-scores)\n","\n","def display_scores(scores):\n","    print(\"Scores:\", scores)\n","    print(\"Mean:\", scores.mean())\n","    print(\"Standard deviation:\", scores.std())\n","\n","display_scores(tree_rmse_scores)\n","\n","lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n","                             scoring=\"neg_mean_squared_error\", cv=10)\n","lin_rmse_scores = np.sqrt(-lin_scores)\n","display_scores(lin_rmse_scores)\n","\n","\"\"\"**Note**: we specify `n_estimators=100` to be future-proof since the default value is going to change to 100 in Scikit-Learn 0.22 (for simplicity, this is not shown in the book).\"\"\""],"metadata":{"id":"zYRzDp77viIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n","forest_reg.fit(housing_prepared, housing_labels)\n","\n","housing_predictions = forest_reg.predict(housing_prepared)\n","forest_mse = mean_squared_error(housing_labels, housing_predictions)\n","forest_rmse = np.sqrt(forest_mse)\n","forest_rmse\n","\n","from sklearn.model_selection import cross_val_score\n","\n","forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n","                                scoring=\"neg_mean_squared_error\", cv=10)\n","forest_rmse_scores = np.sqrt(-forest_scores)\n","display_scores(forest_rmse_scores)\n","\n","scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n","pd.Series(np.sqrt(-scores)).describe()\n"],"metadata":{"id":"3OAlduIfvlcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### WARN: Must have ###\n","### WARN: the below lines are to produce same result across many runs\n","os.environ['PYTHONHASHSEED']=str(1)\n","import random\n","def reset_random_seeds():\n","   os.environ['PYTHONHASHSEED']=str(1)\n","   np.random.seed(1)\n","   random.seed(1)\n","### WARN: Must have ###\n"],"metadata":{"id":"3VYPeLxSvqxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################\n","### ★★★ YOUR FIRST MODEL ★★★ ###   \n","from sklearn.svm import SVR\n","\n","svm_reg = SVR(kernel='linear', C=1.0, epsilon=0.1)\n","svm_reg.fit(housing_prepared, housing_labels)\n","housing_predictions = svm_reg.predict(housing_prepared)\n","svm_mse = mean_squared_error(housing_labels, housing_predictions)\n","svm_rmse = np.sqrt(svm_mse)\n","svm_rmse\n","\n","from sklearn.model_selection import cross_val_score\n","# these below parameters shall be kept unchanged\n","svm_scores = cross_val_score(svm_reg, housing_prepared, housing_labels,\n","                                scoring=\"neg_mean_squared_error\", cv=3) \n","svm_rmse_scores = np.sqrt(-svm_scores)\n","display_scores(svm_rmse_scores)     # printing scores to check during running\n","# capture the below result in the report"],"metadata":{"id":"JubRUk-Dv-UW","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"error","timestamp":1679397185835,"user_tz":-540,"elapsed":1013,"user":{"displayName":"신성한","userId":"02352508022902564974"}},"outputId":"ad5d5924-943c-4f46-ef27-c0d1946fcc5d"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9f9538db04f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvm_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msvm_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_prepared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mhousing_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_prepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msvm_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'housing_prepared' is not defined"]}]},{"cell_type":"code","source":["### WARN: Must have ###\n","### WARN: the below lines are to produce same result across many runs\n","os.environ['PYTHONHASHSEED']=str(1)\n","import random\n","def reset_random_seeds():\n","   os.environ['PYTHONHASHSEED']=str(1)\n","   np.random.seed(1)\n","   random.seed(1)\n","### WARN: Must have ###"],"metadata":{"id":"szaaBGB-v-Y_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################\n","### ★★★ YOUR SECOND MODEL ★★★ ###   \n","from sklearn.●▲■ import ●▲■\n","\n","your_reg = ●▲■\n","your_reg.fit(housing_prepared, housing_labels)\n","your_housing_predictions = your_reg.predict(housing_prepared)\n","your_mse = mean_squared_error(housing_labels, housing_predictions)\n","your_rmse = np.sqrt(your_mse)\n","your_rmse\n","\n","from sklearn.model_selection import cross_val_score\n","# these below parameters shall be kept unchanged\n","your_scores = cross_val_score(your_reg, housing_prepared, housing_labels,\n","                                scoring=\"neg_mean_squared_error\", cv=3) \n","your_rmse_scores = np.sqrt(-your_scores)\n","display_scores(your_rmse_scores)    # printing scores to check during running\n","\n","#################################\n","if your_rmse_scores.mean() < svm_rmse_scores.mean():\n","    print('SUCCESS! Your RMSE Improvement: {0:0.3f}'.format(\n","                        (svm_rmse_scores.mean() - your_rmse_scores.mean())))\n","else:\n","    print('TRY DIFFERENTLY! Your RMSE Decrease: {0:0.3f}'.format(\n","                        (svm_rmse_scores.mean() - your_rmse_scores.mean())))\n","#################################\n","# ★★★ capture the below result in the report ★★★\n","\n","    \n","\"\"\"In your report, the above result should be clearly written.\n","Also, describe the test environment and library versions.\n","\"\"\"\n","print(\"sklearn version\", sklearn.__version__)\n","print(\"python version\", sys.version)"],"metadata":{"id":"Y6KQ3UzZv_Os"},"execution_count":null,"outputs":[]}]}